---
title: 'Tweeter sentiment analysis during COVID19 pandemic'
author: "Ivan Kozyryev"
date: "4/30/2020"
output: html_document
editor_options: 
  chunk_output_type: inline
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Overview

The goal of my project was to determine whether the overall sentiment of the tweets with COVID19-related hashtags influenced whether the tweet would be retweeted and how many times. My initial hypothesis was that the more negative the tweet is the more retweets it will get because it has potential to stir emotions in many people. During the course of this project as I started to learn more about tweeter, I became more broadly interested in what parameters of the tweet effect the number of retweets. Specifically, I considered the following tweet variables:

1. number of followers
2. number of friends 
3. number of hashtags 
4. number of user mentions 
5. sentiment of the text

While there 90 different variable associated with each tweet and many possibilities for further manipulation of the features, I chose to focus on the five above for two reasons:

* Based on my intuition about social media it seemed likely that those parameters might matter.
* I wanted to have a simpler model that can have interpretable implications about how one can spread positive sentiments and correct information on social media to a wide audience. Or alternatively, understand how *misinformation* related to the disease is effectively spread on social media. 

# Exploratory initial analysis

First let's load all the libraries needed for the analysis of Twitter data. I used [rtweet](https://rtweet.info/) library to interface with the tweeter API. 

```{r message=FALSE}
source('Load_libraries.R')
```

As a first step, let's quickly check what topics are trending on Twitter now. I first get the current trends in the US, then sort all the topics by the tweet volume and look at the top 10 trending topics. 

```{r}
US_trends <- get_trends("United States")

US_trends_sorted <- US_trends %>%
  group_by(trend) %>%
  summarize(tweet_vol = mean(tweet_volume)) %>%
  arrange(desc(tweet_vol))

US_trends_sorted$trend[1:10]
```

Many of the days recently, covid-19 related hashtags were among the top 10 trending topics on Twitter which indicates its importance in the social discussion.

# Obtaining the data for tweets

Let's read *original* tweets and see what associated data is available for them. Thus, I have excluded all replies, retweets and quotes. I also only focused on the tweets in English since I am interested in analyzing the sentiment of their text. 

```{r}
covid19_st <- search_tweets("#COVID19 -filter:retweets -filter:quote -filter:replies",n = 54000, include_rts = FALSE, lang = "en",  retryonratelimit = TRUE) 
colnames(covid19_st)[1:25]
```
As we can see, there is a lot of parameters associated with each tweet but for now I am interested in only a few of those. There are 90 different parameters but I only displayed 25 for example above. Number of display columns can be modified to see all the available data types for each tweet.
```{r}
covid_twts <- covid19_st[,c("status_id","screen_name","text","retweet_count","followers_count","friends_count","mentions_user_id","hashtags","urls_url")]
```

## Pre-processing the text for tweets
In order to perform analysis of the text, I first need to clean up the body of the tweet:

* Separate the text from other tweet parameters
* Remove URLs
* Only keep the upper and lower case characters

```{r}
twt_txt <- covid_twts$text # get the tweet text
head(twt_txt)
twt_txt_no_url <- rm_twitter_url(twt_txt)
# head(twt_txt_no_url)
twt_chrs <- gsub("[^A-Za-z]"," ", twt_txt_no_url)
head(twt_chrs)
```

As we can see by comparing the same tweets in the "raw" form and after formatting, we are left only with letters that can be further processed and analyzed. 

## Pre-processing other tweet-related data
The data we read in provides the lists of the hashtags and users mentioned in each tweet, however, I was interested in looking into how the number of hashtags and mentions effects the degree of retweeting which required some initial processing. 

First, lets count the number of mentions and hashtags
```{r}
twts_tidy <- covid_twts %>%
  mutate(text = twt_chrs) %>%
  mutate(mentions = lengths(mentions_user_id)) %>%
  mutate(hashtags = lengths(hashtags)) %>%
  mutate(urls = lengths(urls_url))
```
It is important to notice that most of the tweets don't have any mentions with "NA" in place so we need to properly account for that. I first found which tweets don't have any associated mentions of users and then properly corrected the "mentions" count for those entries. 
```{r}
mentions_NAinds <- which(is.na(covid_twts$mentions_user_id)) # find which mentions are non-existent
twts_tidy$mentions[mentions_NAinds] <- 0 # correct the number of mentions entries for those indeces
```
Unlike for mentions, note that each tweet has at least one hashtag since we specifically searched for #COVID19 tweets to begin with. But we also need to account for the fact that not all tweets have mentioned urls:
```{r}
urls_NAinds <- which(is.na(covid_twts$urls_url)) # find which urls are non-existent
twts_tidy$urls[urls_NAinds] <- 0 # correct the number of mentions entries for those indeces
```

Just for convinience, let's rename the columns of the tweet data set and remove some of the columns that are redundant or will not be useful for our analysis. 
```{r}
twts_tidy <- twts_tidy %>% 
  dplyr::rename(id = status_id, retweets = retweet_count, friends = friends_count, followers = followers_count)

twts_tidy <- twts_tidy %>% 
  dplyr::select(-c(screen_name,mentions_user_id,urls_url))

colnames(twts_tidy)
```

### Text analysis
In order to study the sentiment of individual tweets we need to unnest tokens (words in this case) and remove stop words that don't carry additional information.
```{r}
twts_clean <- twts_tidy %>%
  unnest_tokens(word,text) %>%
  anti_join(get_stopwords(), by = "word")
```
Let's look at the most common words used in tweets associated with #COVID19
```{r}
common_words <- twts_clean %>%
  count(word) %>%
  arrange(desc(n)) %>%
  top_n(n=25, wt = n)

common_words$word
```
While many of the top words listed have important meaning (like "us" or "people"), there additional words or letter that don't provide any useful information. In the next step, I remove unique stop words that don't add any meaning. 
```{r}
uniq_sw <- data.frame(word = c("s","covid","amp","t", "via", "m", "re", "don", "ve", "q", "gt", "o", "pm"))
# 
twts_clean <- twts_clean %>% 
   anti_join(uniq_sw, by = "word")
```
Notice that I also removed "covid" since all the tweets have it because we specifically searched for #COVID19 and otherwise the wordcloud would be overwhelmed by the "covid" word (hashtag symbol and "19" were removed during the pre-processing step for the text).

# Visualizing the most common words
Let's see which words are the most popular in tweets after we cleanup the text data. 
```{r}
pal <- brewer.pal(8,"Dark2")

twts_clean %>%
  count(word) %>%
  with(wordcloud(word,n,random.order = FALSE, max.words = 100, colors = pal))
```

# Sentiment analysis of the #COVID19 tweets 
From the wordcloud above, we see that some of the most mentioned words are positive in meaning while others are very negative in meaning. We are now ready to perform tweet sentiment analysis to see whether the *overall* tweet sentiment influences the number of retweets (and therefore potentially the influence of a tweet on other users). In the first step I used "afinn" lexicon which assigned a positive or negative score to each word. The reason I decided to use afinn lexicon is that it doesn't purely flag the word as "positive" or "negative" but assigns degrees to how positive or negative each word is.. 

Now lets assign a quantitative score to tweets to determine whether they are positive or negative. But first we need to determine which words from the afinn database appear in the tweets we are processing. 

```{r}
twts_afinn <- twts_clean %>%
  inner_join(get_sentiments("afinn"), by = "word")
```
Lets assign the sentiment score to each unique tweet
```{r}
afinn_sentiment <- twts_afinn %>%
  group_by(id,retweets,followers,friends,hashtags,mentions,urls) %>%
  summarize(score = sum(value)) %>%
  arrange(desc(score))
```
Let's look at some of the most positive tweets to get inspiration for your day (and make sure that our analysis makes sense):
```{r}
covid_twts$text[which(twts_tidy$id == afinn_sentiment$id[1])]
covid_twts$text[which(twts_tidy$id == afinn_sentiment$id[2])]
covid_twts$text[which(twts_tidy$id == afinn_sentiment$id[3])]
```
Those are indeed positive tweets, so it makes sense that they received high ranking. 

We can also look at the most negative tweets (according to the afinn lexicon). However, I am going to suppress the output here because it is usually very negative and sometimes not appropriate. 
```{r eval = FALSE}
#covid_twts$text[which(twts_tidy$id == afinn_sentiment$id[length(afinn_sentiment$id)])]
#covid_twts$text[which(twts_tidy$id == afinn_sentiment$id[length(afinn_sentiment$id)-1])]
#covid_twts$text[which(twts_tidy$id == afinn_sentiment$id[length(afinn_sentiment$id)-2])]
```
Those tweets do have negative sentiments and the ranking according to the afinn lexicon makes intuitive sense. 

# Regression analysis for binary retweet status

As first step, lets see what factors determine whether the tweet is retweeted at all. 

## Logistic regression

Change the number of retweets into a binary indicator form: "N"(no retweets)/"A"(any retweet)
```{r}
afinn_binary <- afinn_sentiment %>%
  mutate(retwtBi = ifelse(retweets > 0, "A", "N"))
```

Convert the retweet indicator into a ranked factor: N < A

```{r}
afinn_binary <- afinn_binary %>%
  mutate(retwtBi = factor(retwtBi, levels = c("N", "A")))
```

Let's look at the distribution of the tweets into the "None" and "Any" retweet categories. 

```{r}
afinn_binary %>%
  ggplot(aes(retwtBi)) +
  geom_bar() +
  xlab("Retweet category") +
  ggtitle("Distribution of the #COVID19 tweets into retweet categories")
```

As expected, most of the tweets are not reptweeted at all, but a significant portion got retweeted at least ones. In fact we can see how many tweets got retweeted: 
```{r}
summaryBi <- afinn_binary %>%
  group_by(retwtBi) %>%
  summarize(n = n())
summaryBi
```

For this given run, looks like `summaryBi$n[2]/sum(summaryBi$n)*100`% of `sum(summaryBi$n)` tweets I am analyzing have been retweeted. 

### A brief intro to logistic regression
The probability that a tweet is retweeted is $0<p(A)<1$.We can model such a probability distribution with a logistic function:
$$ \sigma(t)=\frac{1}{1+e^{-t}} $$  
which has the following important properties:

* $\sigma(t)\rightarrow 0$ as $t\rightarrow -\infty$
* $\sigma(t)\rightarrow 1$ as $t\rightarrow \infty$
* $\sigma(t)=1/2$ for $t=0$

Therefore, it seems reasonable to model the probability $p(A)$ as:

$$ p = \frac{\text{exp}(\beta_0+\beta_1x_1+\beta_2x_2+\cdots\beta_kx_k)}{1+\text{exp}(\beta_0+\beta_1x_1+\beta_2x_2+\cdots\beta_kx_k)} $$
which is equivalent to the expression for $\sigma(t)$ above. Therefore, the odds are 

$$ \frac{p}{1-p} = \text{exp}(\beta_0+\beta_1x_1+\beta_2x_2+\cdots\beta_kx_k)$$

but more importantantly $logit(p)$ is 

$$ \text{log} \left(\frac{p}{1-p}\right)=\beta_0+\beta_1x_1+\beta_2x_2+\cdots\beta_kx_k $$         

is linear in the fitting coefficients $\beta_k$. Alternatively, equation above can be represented as:

$\text{log}\frac{p(A)}{p(N)} = \beta_0 + \beta^Tx$

where $p(A)+p(N) = 1$. 
 
### Implementing logistic regression

Before we can we train the logistic regression model, we need to split the dataset into train and test subsets; here I used the 60/40 split ratio.
```{r}
trBi <- sample(nrow(afinn_binary),round(nrow(afinn_binary)*0.6))
trainSet <- afinn_binary[trBi,]
testSet <- afinn_binary[-trBi,]
```

First, we fit the logit model to the training set to see which parameters are statistically significant:
```{r warning = FALSE}
model1LR <- glm(retwtBi ~ followers + friends + score + mentions + hashtags + urls, data = trainSet, family = "binomial")
summary(model1LR)
```

In order to determine how accurate the fitted model is we now apply it to the "test" dataset and compare to the actual values:
```{r}
prob1LR <- predict(model1LR,testSet, type = "response")
# prob1LR
cl1LR <- ifelse(prob1LR > 0.5, "A", "N")
```

Confusion matrix will provide a lot of valuable information on the model peformance. I set the positive class to "A" as we are interested in predicting whether the tweet with receive any retweets

```{r}
confusionMatrix(factor(cl1LR, levels = c("N","A")),testSet$retwtBi, positive = "A")
```
From the summary table of logistic regression model (model1LR) we can see that followers and mentions variables have the highest statistical significance in determining whether the tweet will be retweeted at least once or not. Let's run the second model now only with those parameters
```{r warning = FALSE}
model2LR <- glm(retwtBi ~ followers + mentions, data = trainSet, family = "binomial")
summary(model2LR)
```
We can see now that all of the variables are statistically significant as expected. 
```{r}
prob2LR <- predict(model2LR,testSet, type = "response") # predicted probability
# pred2LR
cl2LR <- ifelse(prob2LR > 0.5, "A", "N") # predicted binary classification
confusionMatrix(factor(cl2LR, levels = c("N","A")),testSet$retwtBi, positive = "A")
```
We can see that accuracy has in fact slightly improved. 

One of the advantages of the ligistic regression is that it's coefficients lend to intuitive interpretation based on the log(odds) discussion above.
```{r}
exp(coef(model2LR)) 
```
We can see that mentions of other users have the largest effect on log-odds. This is something that the author of the tweet can control thus potentially increasing the odd of the tweet being retweeted

## Decision tree for binary retweet indicator

The logit function assumed a linear relationship between the log(odd) and fitting coefficients. Thus it could be instructive to look at some nonlinear models. Let's use the same variables but fit the decision tree instead of logit regression 
```{r}
model1DT <- rpart(retwtBi ~ followers + friends + score + mentions + hashtags, data = trainSet, method = "class")
rpart.plot(model1DT)
```

```{r}
cl1DT <- predict(model1DT,testSet, type = "class")
confusionMatrix(cl1DT,testSet$retwtBi, positive = "A")
```

As we can see from the confusion matrix, a single decision tree performs better than the logistic regression on such important parameters as accuracy and sensitivity (recall). Therefore, there is a lot for potential for random forest approach to improve the predicting power of our model. 

Let's fit the random forest model to the full data:

```{r}
model1RF <- randomForest(retwtBi ~ followers + friends + score + mentions + hashtags + urls, data = trainSet, importance = TRUE)
```
We can now calculate predicted probabilities and classes:
```{r}
prob1RFboth <- predict(model1RF,testSet, type = "prob") # predicted probabilities 
prob1RF = prob1RFboth[,"A"]
# prob1RF
cl1RF <- predict(model1RF,testSet, type = "class") # predicted binary classification
# cl1RF
confusionMatrix(cl1RF,testSet$retwtBi, positive = "A")
```
In order to compare how different fitted model perform I will look at the ROC curves:

# let's plot the ROC curve
```{r warning = FALSE}
bmrk = seq(0,1,by=0.01)
caTools::colAUC(cbind(prob1LR,prob2LR,prob1RF, bmrk), testSet$retwtBi,plotROC = T)
```

Area under the ROC curve can be used to compare various models in order to pick the optimal one and we conclude that random forest model performs the best on this metric as well. I also plotted the benchmark line for reference.

From the area under the ROC curve we can see that Logistic Regression model 2 (LR2) (fitted only on followers and mentions) performs better compared to LR1 model (fitted on all data). We can use cross-validation to try fitting different logistic regression models and seeing which one performs the best:  

```{r warning = FALSE}
set.seed(42)
# use cross-validation from the caret library
cv_model1 <- train(
  retwtBi ~ followers,
  data = trainSet,
  method = "glm",
  family = "binomial",
  trControl = trainControl(method = "cv", number = 10)
)

cv_model2 <- train(
  retwtBi ~ mentions,
  data = trainSet,
  method = "glm",
  family = "binomial",
  trControl = trainControl(method = "cv", number = 10)
)

cv_model3 <- train(
  retwtBi ~ hashtags,
  data = trainSet,
  method = "glm",
  family = "binomial",
  trControl = trainControl(method = "cv", number = 10)
)

cv_model4 <- train(
  retwtBi ~ score,
  data = trainSet,
  method = "glm",
  family = "binomial",
  trControl = trainControl(method = "cv", number = 10)
)

cv_model5 <- train(
  retwtBi ~ followers + mentions + hashtags,
  data = trainSet,
  method = "glm",
  family = "binomial",
  trControl = trainControl(method = "cv", number = 10)
)
```

Let's comapre the accuracy performance of different models now:
```{r}
summary(
  resamples(
    list(
      cv1 = cv_model1,
      cv2 = cv_model2,
      cv3 = cv_model3,
      cv4 = cv_model4,
      cv5 = cv_model5
    )
  )
)$statistics$Accuracy
```

We can see that the model with all three statistically significant parameters from fitting model LR2 has the best accuracy. However, using only the number of followers in the logistic regression model provides a decent accuracy as well, which indicates that this is the most important parameter for predicting whether the tweet is retweeted or not. 

## Summary of the Binary Retweet Analysis
We have used Logistic Regression to identify the most important tweet variables in determining whether it is retweeted or not. Such factors as number of followers, mentions of other users and number of hashtags turned out to be statiscatilly important. We further observed that number of **followers** is the most important mentric in predicting the propensity of the tweet to be retweeted. 

We can try adding more information about the tweet to begin with (remember there are 90 different variables associated with a single tweet we can get) however, personally I am interested in seeing which tweets influence many people: i.e. what causes tweet to become "viral" and be retweeted hundreds or thousands of times and potentially have a significant influence on the perception of many people about COVID19. Thus, let's actually divide tweets which have been retweet into multiple sub-categories. 


# Multivariate Logit 

First I changed the retweets into 4 categories (CAT) that define how "popular" the tweet is: 

* "N (none)":0 
* "S (small)":1-10 
* "M (medium)":11-100 
* "L (large)":>100 

While other possible divisions exist (for example I included "viral" category initially with >1,000 retweets), I wanted to make sure that there are enough samples in each category for statistically significant results from the analysis. 
```{r}
# afinn_five <- afinn_sentiment %>%
   # mutate(retwtCAT = ifelse(retweets == 0, "N",ifelse(retweets %in% 1:10, "S", ifelse(retweets %in% 11:100, "M",ifelse(retweets %in% 101:500, "L","V")))))

# In order to do the random forest regression we cannot have any empty categories; so delete the Viral category since it is often empty
afinn_four <- afinn_sentiment %>%
  mutate(retwtCAT = ifelse(retweets == 0, "N",ifelse(retweets %in% 1:10, "S", ifelse(retweets %in% 11:100, "M", "L"))))
```

Lets convert the categories for retweets into fanked factors since there is an inherent order to them this order will be used with the multivariate logistic regression or can be used for ordinal logistic regression. 
```{r}
# afinn_RK <- afinn_five %>%
#    mutate(retwtCAT = factor(retwtCAT,levels = c("N","S","M","L","V")))
# In case "L" category is empty use only 4 buckets
afinn_RK <- afinn_four %>%
 mutate(retwtCAT = factor(retwtCAT,levels = c("N","S","M","L")))
```
Let's see how many tweets per category there are in our sample:
```{r}
afinn_RK %>%
  ggplot(aes(retwtCAT)) +
  geom_bar() +
  xlab("Retweet category") +
  ggtitle("Distribution of the #COVID19 tweets into retweet categories")
```

Most of the tweets appear to come from "N" and "S" categories which we should keep in mind when trying to predict tweets which will be retweeted more than 10 times. This makes sense intuitively since we are looking at the tweets posted recently only. 

Let's display the sentiment distribution for each category
```{r}
afinn_RK %>%
  ggplot(aes(retwtCAT,score)) +
  geom_boxplot() +
  xlab("Retweet category") +
  ylab("Sentiment score") +
  ggtitle("Sentiment distribution of #COVID19 tweets for each retweet category")
```

Let's perform the multinomial logistic regression next since our oucome can be in one of the 4 categories: "N", "S", "M" or "L". 

```{r}
modelMLR_A <- multinom(retwtCAT ~ followers + friends + score + mentions + hashtags + urls, data = afinn_RK)
summary(modelMLR_A)
```
Notice that for all the fit coefficients are referenced to the "N" category:
$$ \text{log} \frac{Pr(CAT)}{Pr(N)} = \beta_0 + \beta^Tx$$ 
where $CAT$ is "S", "M" or "L". 
Let's estimate the statistical significance of the fit coefficients using the p-value for Wald's test:
```{r}
z <- summary(modelMLR_A)$coefficients/summary(modelMLR_A)$standard.errors

z 

p <- (1 - pnorm(abs(z), 0, 1)) * 2
p
```

It looks like coefficients for followers, friends, sentiment score, mentions, hashtags and urls are statistically significant. Let's now look in more details at each category but first we need to add additional binary indicators to the dataset:

```{r}
afinn_RK <- afinn_RK %>%
  mutate(isSmall = ifelse(retwtCAT == "S", 1, 0)) %>%
  mutate(isMedium = ifelse(retwtCAT == "M", 1, 0)) %>%
  mutate(isLarge = ifelse(retwtCAT == "L", 1, 0))
```
Split the data into train and test sets:
```{r}
tr2 <- sample(nrow(afinn_RK),round(nrow(afinn_RK)*0.6)) # split into training and test subsets
trainRK <- afinn_RK[tr2,]
testRK <- afinn_RK[-tr2,]
```

Let's see how many tweets per category there are in our training and test samples to make sure that the distributions are representative of the total sample:
```{r}
trainRK %>%
  ggplot(aes(retwtCAT)) +
  geom_bar() +
  xlab("Retweet category for training set") +
  ggtitle("Distribution of the #COVID19 tweets into retweet categories for training set")

testRK %>%
  ggplot(aes(retwtCAT)) +
  geom_bar() +
  xlab("Retweet category for test set") +
  ggtitle("Distribution of the #COVID19 tweets into retweet categories for test set")
```

As we can see from the histograms, the relative distributions look very similar, which provides strong indication that the best model training and benchmarked will be extendable to other future tweet analysis on this topic.  

```{r warning = FALSE}
modelRK_LR1 <- glm(isSmall ~ followers + friends + hashtags + mentions + urls + score, data = trainRK, family = binomial(link="logit"))
summary(modelRK_LR1)
```

```{r warning = FALSE}
modelRK_LR2 <- glm(isMedium ~ followers + friends + hashtags + mentions + urls + score, data = trainRK, family = binomial(link="logit"))
summary(modelRK_LR2)
```
```{r warning = FALSE}
modelRK_LR3 <- glm(isLarge ~ followers + friends + hashtags + mentions + urls + score, data = trainRK, family = binomial(link="logit"))
summary(modelRK_LR3)
```


Let's look at the confusion matrices for "S", "M" and "L" categories:

```{r}
probS_LR1 <- predict(modelRK_LR1,testRK, type = "response")
clS_LR1 <- ifelse(probS_LR1 > 0.5, 1, 0) # predicted binary classification
confusionMatrix(factor(clS_LR1), factor(testRK$isSmall),positive = '1')
```
```{r}
probM_LR2 <- predict(modelRK_LR2,testRK, type = "response")
clM_LR2 <- ifelse(probM_LR2 > 0.5, 1, 0) # predicted binary classification
confusionMatrix(factor(clM_LR2), factor(testRK$isMedium),positive = '1')
```

```{r}
probL_LR3 <- predict(modelRK_LR3,testRK, type = "response")
clL_LR3 <- ifelse(probL_LR3 > 0.5, 1, 0) # predicted binary classification
confusionMatrix(factor(clL_LR3), factor(testRK$isLarge),positive = '1')
```

It is important to note that while the accuracy for the logit model to predict "L" category is the highest, it is not a useful metric in this case before there are very little "L" category tweets. Therefore, we should focus on sensitivity and specificity instead as useful metrics for predicing categories with "S", "M" and "L" indicators. 

# Decision trees to determine retweet categories --------------------------

Before we proceed with model fitting analysis we need to split the retweet categorical data into train and test subsets:
```{r}
modelRK_DT <- rpart(retwtCAT ~ followers + friends + score + mentions + hashtags + urls, data = trainRK, method = "class")

predRK_DT <- predict(modelRK_DT,testRK, type = "class")

rpart.plot(modelRK_DT)
confusionMatrix(predRK_DT, testRK$retwtCAT)
```
In fact it looks like a simple decision tree performs much better in predicting tweets in "S" and "M" categories. By comparing the $F_1 = \frac{2}{1/sensitivity + 1/specificity}$ score of the decision tree fit with the one-vs-all logistic regression models above, we can see that a nonlinear model performs much better here.  

** KNN classifier

I was interested in seeing how k-nearest-neighbors classifier will perform on this multivariate classification challenge for retweet categories. However, before we can efficiently apply it on our tweet data set, I needed to transform the tweet features since they are of very diferent scales: while mentions and urls are usually a small number, followers can be in the hundreds or thousands. Moreover, sentiment score can be positive or negative while all the other variables can only take values larger than zero. Thus, I rescaled and centered the features before applying knn classification:

```{r}
trCtrl <- trainControl(method = "repeatedcv", number = 10, repeats = 3)

knn_fit <- train(retwtCAT ~ followers + friends + score + mentions + hashtags + urls, data = trainRK, method = "knn",
                 trControl = trCtrl,
                 preProcess = c("center","scale"),
                 tuneGrid = expand.grid(k = c(2,5,10,20,30,40,50,60)))

```
Let's look at the results now:
```{r}
knn_fit
plot(knn_fit)
```

It looks like using 30 neighbors provides the highest accuracy. However, we need to see how out model performs on the test data. 

```{r}
test_pred <- predict(knn_fit, testRK)
confusionMatrix(test_pred, testRK$retwtCAT)
```

Caret library makes it easy to try other machine learning methods. RandomForest performed well for the binary classification problem whether tweet will get retweeted or not so I excpeted it to do well here to. Let's see how it performs. Notice that I chose not too rescale the features since randomforest should be robust against different scales in the variables.

```{r}
trCtrl <- trainControl(method = "repeatedcv", number = 10, repeats = 3, search = 'random')

rf_fit <- train(retwtCAT ~ followers + friends + score + mentions + hashtags + urls, data = trainRK, method = "rf",
                 trControl = trCtrl,
                 metric = 'Accuracy',
                 tuneLength = 10)

```
Let's look at the results:

```{r}
rf_fit
plot(rf_fit)
```
Let's see what the out-of-sample error is:

```{r}
test_pred <- predict(knn_fit, testRK)
confusionMatrix(test_pred, testRK$retwtCAT)
```


